{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c61e1ec-bda5-49a7-9620-45a8de8209d1",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955ff337-e8f7-4667-91fd-9372c12e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab6a85-7c41-45f9-845b-cf5bd4f85b7c",
   "metadata": {},
   "source": [
    "<h3> Load the Pre-trained VGG16 Model and Add Custom Classification Layers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c0f001-468d-4fad-9951-ddd99cf2815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes for the food recognition model\n",
    "num_of_foods = 12\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet, without the top layer\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',       # Use weights pre-trained on ImageNet\n",
    "    include_top=False,        # Do not include the top (fully connected) layers\n",
    "    input_shape=(224, 224, 3) # Input shape must match VGG16's requirements\n",
    ")\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during training\n",
    "base_model.trainable = False\n",
    "# Functional API approach to build the model\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)       # Pass inputs through the VGG16 model\n",
    "\n",
    "# Add global average pooling to reduce dimensions\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense layer for further learning\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Output layer with softmax activation for multi-class classification\n",
    "outputs = layers.Dense(num_of_foods, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb5d98-9cdd-4fab-b2c8-f1508575ed9a",
   "metadata": {},
   "source": [
    "<h3>Compile the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f158b2-d944-434d-9004-f8a33dd86966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,548</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │         \u001b[38;5;34m1,548\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,781,900</span> (56.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,781,900\u001b[0m (56.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,212</span> (262.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,212\u001b[0m (262.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2eb57-c3fe-43fc-bc7e-12096f42fa0e",
   "metadata": {},
   "source": [
    "<h3>Data Generation and Pre-Processing</h3>\n",
    "<h4>Use ImageDataGenerator to load and preprocess the images.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4d7b60-0e90-496c-ade5-0402edcb44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5038 images belonging to 12 classes.\n",
      "Found 1254 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,        # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    rotation_range=20,     # Data augmentation options\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2   # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Path to your training dataset directory\n",
    "data_dir = 'D:/food_dataset'   # Replace with the actual path\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),  # Resize images to match VGG16 input size\n",
    "    batch_size=18,\n",
    "    class_mode='sparse',     # Use 'sparse' if labels are integers\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Create validation data generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ad54c-7472-49df-a384-61d3f7db1683",
   "metadata": {},
   "source": [
    "<h3>Train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f205c12-de55-48ec-b3b7-f9aa25023b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1958s\u001b[0m 7s/step - accuracy: 0.3795 - loss: 1.9758 - val_accuracy: 0.6619 - val_loss: 1.2579\n",
      "Epoch 2/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3124s\u001b[0m 11s/step - accuracy: 0.6569 - loss: 1.1490 - val_accuracy: 0.7344 - val_loss: 0.9482\n",
      "Epoch 3/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3185s\u001b[0m 11s/step - accuracy: 0.7357 - loss: 0.9087 - val_accuracy: 0.6595 - val_loss: 0.9421\n",
      "Epoch 4/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3178s\u001b[0m 11s/step - accuracy: 0.7575 - loss: 0.8003 - val_accuracy: 0.7600 - val_loss: 0.7841\n",
      "Epoch 5/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3157s\u001b[0m 11s/step - accuracy: 0.7802 - loss: 0.7085 - val_accuracy: 0.7584 - val_loss: 0.7406\n",
      "Epoch 6/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3272s\u001b[0m 11s/step - accuracy: 0.7909 - loss: 0.6523 - val_accuracy: 0.7616 - val_loss: 0.6930\n",
      "Epoch 7/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3558s\u001b[0m 12s/step - accuracy: 0.8088 - loss: 0.6119 - val_accuracy: 0.7982 - val_loss: 0.6241\n",
      "Epoch 8/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3064s\u001b[0m 11s/step - accuracy: 0.8062 - loss: 0.5848 - val_accuracy: 0.7935 - val_loss: 0.6662\n",
      "Epoch 9/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2974s\u001b[0m 10s/step - accuracy: 0.8240 - loss: 0.5676 - val_accuracy: 0.8006 - val_loss: 0.6141\n",
      "Epoch 10/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2958s\u001b[0m 10s/step - accuracy: 0.8278 - loss: 0.5342 - val_accuracy: 0.8206 - val_loss: 0.5464\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # You can change the number of epochs based on your requirements\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a8611-73c0-4528-a125-c4afc8071de7",
   "metadata": {},
   "source": [
    "<h3>Evaluate the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188fdf3c-8524-465a-8015-cd3f3d755f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 11s/step - accuracy: 0.8218 - loss: 0.5280\n",
      "Validation Accuracy: 81.26%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e4190-8277-4b7e-b604-6813f26a5717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
